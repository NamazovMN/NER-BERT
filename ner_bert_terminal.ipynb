{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Description**\n",
        "\n",
        "If you are here, it means, you already checked GitHub repo of this project, so that you have enough information about Named Entity Recognition and what did we do in the project. In case you reached out this ner_bert.ipynb file randomly, you can reach to the [repo](https://github.com/NamazovMN/NER-BERT) easily, to check  the source code and a bit more information.\n",
        "\n",
        "This file is to show how can you easily use the repository in your own local machine or in any notebook supported environment!\n",
        "\n",
        "Hope you will enjoy it!"
      ],
      "metadata": {
        "id": "uAYvyTXtlJtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Initial Steps**"
      ],
      "metadata": {
        "id": "xl_zQmc_mgna"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Drive"
      ],
      "metadata": {
        "id": "eDM274qzmuF2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hi1BEWpalG66",
        "outputId": "fbf3481f-c8aa-4867-abc4-74724355fd99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Directory\n",
        "In order to prevent messed up data organization in you drive, we will create directory, where we will clone the repository:"
      ],
      "metadata": {
        "id": "QjsbT3q6m_Ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "folder_name = 'projects_clone' #change it as you wish\n",
        "path = os.path.join('drive/MyDrive/Colab Notebooks/', folder_name)\n",
        "if not os.path.exists(path):\n",
        "  os.makedirs(path)\n",
        "  %cd -q $path\n",
        "else:\n",
        "  %cd -q $path"
      ],
      "metadata": {
        "id": "vm5UHy_gm9Xt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clone the project into the given path\n",
        "Now we are in the directory of 'drive/MyDrive/Colab Notebooks/[folder_name]', where folder_name can be anything that you can modify from the cell above. Let's clone the project:"
      ],
      "metadata": {
        "id": "dypqP0aXpOw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'NER-BERT' not in os.listdir():\n",
        "  !git clone https://github.com/NamazovMN/NER-BERT.git\n",
        "project_path = 'NER-BERT'\n",
        "%cd -q $project_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d77h3vzbpNdK",
        "outputId": "b0e90f60-2c6d-40f8-c4d6-a5bd7a04d83d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NER-BERT'...\n",
            "remote: Enumerating objects: 68, done.\u001b[K\n",
            "remote: Counting objects: 100% (68/68), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 68 (delta 17), reused 63 (delta 12), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (68/68), 1.31 MiB | 11.90 MiB/s, done.\n",
            "Resolving deltas: 100% (17/17), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's install dependencies\n",
        "Now we are kind of set, let's install required dependencies through [requirements.txt](https://github.com/NamazovMN/NER-BERT/blob/main/requirements.txt).\n"
      ],
      "metadata": {
        "id": "YOaV0H26rNVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFBuL3PxpGbu",
        "outputId": "4384636a-e5d3-470d-b57f-437438fa1271"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.0.1+cu118)\n",
            "Requirement already satisfied: transformers~=4.24.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (4.24.0)\n",
            "Requirement already satisfied: scikit-learn~=1.2.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.2.2)\n",
            "Requirement already satisfied: tqdm~=4.65.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.65.2)\n",
            "Requirement already satisfied: datasets~=2.13.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (2.13.1)\n",
            "Requirement already satisfied: pandas~=1.5.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.5.3)\n",
            "Requirement already satisfied: nltk~=3.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (3.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch~=2.0.0->-r requirements.txt (line 1)) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch~=2.0.0->-r requirements.txt (line 1)) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch~=2.0.0->-r requirements.txt (line 1)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch~=2.0.0->-r requirements.txt (line 1)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch~=2.0.0->-r requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch~=2.0.0->-r requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch~=2.0.0->-r requirements.txt (line 1)) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch~=2.0.0->-r requirements.txt (line 1)) (16.0.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.24.0->-r requirements.txt (line 2)) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.24.0->-r requirements.txt (line 2)) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.24.0->-r requirements.txt (line 2)) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.24.0->-r requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.24.0->-r requirements.txt (line 2)) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers~=4.24.0->-r requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.24.0->-r requirements.txt (line 2)) (0.13.3)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn~=1.2.1->-r requirements.txt (line 3)) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn~=1.2.1->-r requirements.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn~=1.2.1->-r requirements.txt (line 3)) (3.2.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets~=2.13.1->-r requirements.txt (line 5)) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets~=2.13.1->-r requirements.txt (line 5)) (0.3.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets~=2.13.1->-r requirements.txt (line 5)) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets~=2.13.1->-r requirements.txt (line 5)) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets~=2.13.1->-r requirements.txt (line 5)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets~=2.13.1->-r requirements.txt (line 5)) (3.8.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas~=1.5.3->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas~=1.5.3->-r requirements.txt (line 6)) (2023.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk~=3.7->-r requirements.txt (line 7)) (8.1.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.13.1->-r requirements.txt (line 5)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.13.1->-r requirements.txt (line 5)) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.13.1->-r requirements.txt (line 5)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.13.1->-r requirements.txt (line 5)) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.13.1->-r requirements.txt (line 5)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.13.1->-r requirements.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.13.1->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas~=1.5.3->-r requirements.txt (line 6)) (1.16.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers~=4.24.0->-r requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers~=4.24.0->-r requirements.txt (line 2)) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers~=4.24.0->-r requirements.txt (line 2)) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch~=2.0.0->-r requirements.txt (line 1)) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch~=2.0.0->-r requirements.txt (line 1)) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**How To Use?**\n",
        "In order to run the model as a script we need to know some details. In this section we will provide them"
      ],
      "metadata": {
        "id": "6DGHcIBxvn2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Project Parameters\n",
        "We need to know what parameters do what. For this you can check the project parameters from below:\n",
        "\n",
        "        \"experiment_num\": 6 => Specifies number of experiment (for folder design)\n",
        "        \"epochs\": 3 => Number of epochs you want to run the training (or fine-tuning)\n",
        "        \"learning_rate\": 0.0001 => Learning rate of the training\n",
        "        \"batch_size\": 16 => Batch size\n",
        "        \"weight_decay\": 0.0001 => Weight decay as a regularizer parameter\n",
        "        \"train\": True => Train the model or not (must be specified)\n",
        "        \"infer\": True => Activate inference session or not (must be specified)\n",
        "        \"resume_training\": False => When you want to continue training phase of specified experiment (with experiment number) (must be specified)\n",
        "        \"epoch_choice\": 1 => specifies which epoch should be chosen (in resume or inference scenarios)\n",
        "        \"load_best\": False => Specifies whether we want to load model based on best metric results\n",
        "        \"load_choice\": 'f1_macro' => Best choice will be set on this choice. While default is 'f1_macro', you can also choose either dev_loss or dev_accuracy\n",
        "        \"dropout\": 0.3 => dropout rate\n",
        "        \"max_length\": 180 => maximum length that will be considered by the model\n",
        "        \"model_checkpoint\": 'bert-base-cased' => model checkpoint that you want to use\n",
        "        \"stats\": True => In case you want to see statistics set it to True, please (must be specified)\n",
        "        \"statistics_data_choice\": 'test' => Results will be done based on this dataset (can be 'test' or 'validation')\n",
        "Note: Parameters convey *(must be specified)* information, are set to False by default. In order make them True (in argparse techniques *store_true*) you must call them."
      ],
      "metadata": {
        "id": "Tu9I19hTv5hP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "In order to train the model following parameters are enough to start: experiment_num, train. Rest of the parameters will be set by default values. But you can also modify themm, if you want. For simplicity, we can say that these default values are okay and we will train the model for 3 epochs (default)"
      ],
      "metadata": {
        "id": "DhGVCK5TyCue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --train --experiment_num 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMykSc5yyhpq",
        "outputId": "cfe5423f-7001-416d-fce0-aff66d64a762"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-21 11:51:48.886662: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-21 11:51:49.996956: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Found cached dataset conllpp (/root/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2)\n",
            "100% 3/3 [00:00<00:00, 543.94it/s]\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-092064d366e0730c.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-685e8d854b3e67e6.arrow\n",
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Project Parameters for experiment 1 were saved successfully!\n",
            "<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>> \n",
            "\n",
            "  0% 0/878 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "Epoch: 1 Loss:  0.1349 Accuracy:  0.9611: 100% 878/878 [06:58<00:00,  2.10it/s]\n",
            "Validation: Loss:  0.0744 Accuracy:  0.9793: 100% 204/204 [00:36<00:00,  5.52it/s]\n",
            "F1 scores => macro:  0.9271, micro:  0.9793\n",
            "Model and Optimizer parameters were saved for epoch 1\n",
            "Epoch results were added to the existing data for epoch 1\n",
            "\n",
            "<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch: 2 Loss:  0.0577 Accuracy:  0.9839: 100% 878/878 [07:07<00:00,  2.05it/s]\n",
            "Validation: Loss:  0.0886 Accuracy:  0.9769: 100% 204/204 [00:36<00:00,  5.52it/s]\n",
            "F1 scores => macro:  0.9208, micro:  0.9769\n",
            "Model and Optimizer parameters were saved for epoch 2\n",
            "Epoch results were added to the existing data for epoch 2\n",
            "\n",
            "<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch: 3 Loss:  0.0431 Accuracy:  0.9880: 100% 878/878 [07:07<00:00,  2.05it/s]\n",
            "Validation: Loss:  0.0903 Accuracy:  0.9779: 100% 204/204 [00:36<00:00,  5.52it/s]\n",
            "F1 scores => macro:  0.9213, micro:  0.9779\n",
            "Model and Optimizer parameters were saved for epoch 3\n",
            "Epoch results were added to the existing data for epoch 3\n",
            "\n",
            "<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>\n",
            "Test: Loss:  0.1544 Accuracy:  0.9624: 100% 216/216 [00:39<00:00,  5.45it/s]\n",
            "F1 scores => macro:  0.8789, micro:  0.9624\n",
            "Test results were saved after training of 3 epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Inference\n",
        "In case you want to check the model results by prompting some text, you need to activate inference phase. As we mentioned above, initially it is set to False and we need to call the parameter in order to activate it.\n",
        "\n",
        "We must emphasize that, in case you do not have any checkpoint to load the model (in other words, you do not have experiment_# folder, where # is experiment number you want to run) inference will not be performed.\n",
        "\n",
        "Let's analyze the scenario that you choose experiment_num 1, and you have the required output results. Then you have to follow some rules:\n",
        "\n",
        "\n",
        "*   If you do not specify any parameters, it will load initial epoch results (epoch_choice which is 1 by default)\n",
        "*   If you specify epoch choice and set load_best to True, the latter will be chosen.\n",
        "*   In case you set load_best to True, you might want to specify load_choice as well. When you choose dev_loss, then the epoch with the minimum loss will be chosen. When you choose either f1_macro or dev_accuracy then the epoch with maximum value for this specified metric will be chosen.\n",
        "\n",
        "For instance in the following example we choose experiment_num as 1 (we already trained the model above), set inference to true by calling it, set load_best to True and make choice as f1_macro.\n",
        "\n",
        "For now, that is all! Have fun!"
      ],
      "metadata": {
        "id": "yx6dEpN-z_Hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --infer --experiment_num 1 --load_best --load_choice f1_macro"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJQHzoX4z7LK",
        "outputId": "681d1340-caed-4eaf-d3dc-92c8d2e96605"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-21 12:34:24.090815: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-21 12:34:25.135475: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Found cached dataset conllpp (/root/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2)\n",
            "100% 3/3 [00:00<00:00, 575.80it/s]\n",
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "WARNING: Best choice and epoch choice were made together! In such cases best choice is prioritized!\n",
            "According to the best choice selection, epoch 1 was chosen!\n",
            "Please provide your text: Chris Nolan made the movie 'Oppenheimer' which was made based on life of J.R.Oppenheimer\n",
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "['Chris', 'Nolan', 'made', 'the', 'movie', \"'Oppenheimer\", \"'\", 'which', 'was', 'made', 'based', 'on', 'life', 'of', 'J.R.Oppenheimer', '[SEP]']\n",
            "[('Chris', 'B-PER'), ('Nolan', 'I-PER'), ('made', 'O'), ('the', 'O'), ('movie', 'O'), (\"'Oppenheimer\", 'O'), (\"'\", 'O'), ('which', 'O'), ('was', 'O'), ('made', 'O'), ('based', 'O'), ('on', 'O'), ('life', 'O'), ('of', 'O'), ('J.R.Oppenheimer', 'I-PER'), ('[SEP]', 'O')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Statistics**\n",
        "\n",
        "In the Named Entity Recognition task, we know that labels are not uniformly distributed. Thus, most of the labels are Others (O). To this end we specified statistics in two directions: With and without O label. That is why, when you run it you will wee 2 graph per chosen metric.\n",
        "\n",
        "On the other hand, we make inference based on 3 metrics of load_choice, but this time they are automatically chosen.\n",
        "\n",
        "Last but not least, dataset choice is made by you thanks to the parameter of *statistics_data_choice*, which can either be test or validation.\n",
        "\n",
        "We have to emphasize, when it is your first time to see statistics, predictions are run based on chosen epoch for each metric and saved to the corresponding experiment folder. Thus, next time you won't need to predict.\n",
        "\n",
        "Let's step into the example then:\n",
        "\n",
        "We set stats parameter to True, and statistics_data_choice to 'validation'. Then you can go to the experiment folder in your drive to check confusion matrices.\n"
      ],
      "metadata": {
        "id": "oEM3-p9L2fZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --stats --statistics_data_choice validation --experiment_num 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kyGYnxO2eKU",
        "outputId": "fad13cbd-13c5-4132-a2c6-874c3b73ca95"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-21 12:44:04.148252: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-21 12:44:05.694131: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Found cached dataset conllpp (/root/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2)\n",
            "100% 3/3 [00:00<00:00, 579.43it/s]\n",
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Figure(1200x1200)\n",
            "Figure(1200x1200)\n",
            "Figure(1200x1200)\n",
            "Figure(1200x1200)\n",
            "Figure(1200x1200)\n",
            "Figure(1200x1200)\n"
          ]
        }
      ]
    }
  ]
}